{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eef6769",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MusicalInformatics/miws23/blob/main/expectation/markov_models_melodic_expectation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "HOME_DIR = \".\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install partitura\n",
    "    !pip install python-hiddenmarkov\n",
    "    !git clone https://github.com/MusicalInformatics/miws23\n",
    "    import sys\n",
    "    sys.path.insert(0, \"/content/miws23/expectation/\")\n",
    "    HOME_DIR = \"/content/miws23/expectation/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e35620ff",
   "metadata": {},
   "source": [
    "# Melodic Expectation with Markov Models\n",
    "\n",
    "In this notebook we will look at Markov Chains for modeling musical expectation.\n",
    "\n",
    "## Quick Recap: Probabilistic Graphical Models\n",
    "\n",
    "* Probabilistic graphical models (PGMs) provide a way to visualize the structure of a probabilistic model\n",
    "\n",
    "* Easy and elegant way to represent conditional independence properties\n",
    "\n",
    "* **Bayesian Networks**: Nodes in a graph represent *random variable* and edges specify conditional independence properties:\n",
    "\n",
    "$$p(X_1, \\dots, X_N) = \\prod_{i=1}^{N} p(X_i \\mid \\mathbf{Pa}(X_i))$$\n",
    "\n",
    "\n",
    "For the example above\n",
    "\n",
    "$$p(X_1, X_2, X_3, X_4, X_5, X_6) = p(X_1) p(X_2) p(X_3\\mid X_1, X_2) p(X_4 \\mid X_3) p(X_5 \\mid X_3) p(X_6 \\mid X_3)$$\n",
    "\n",
    "## Markov Models\n",
    "\n",
    "### Independent Observations\n",
    "* The simplest way of modeling a sequence of observations is to treat them as independent\n",
    "\n",
    "<div>\n",
    "<img src=\"img/independent_markov.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "$$p(\\mathbf{x}_1\\dots, \\mathbf{x}_N) = \\prod_{n=1}^{N} p(\\mathbf{x}_i)$$\n",
    "\n",
    "but this is a poor assumption for inherently sequential data (like music!)\n",
    "\n",
    "### First order Markov Chains\n",
    "\n",
    "The conditional distribution of each variable is independent of all previous observations except for the *most recent*\n",
    "\n",
    "<div>\n",
    "<img src=\"img/first_order_markov.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "$$p(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) = p(\\mathbf{x}_1)\\prod_{n=2}^{N}p(\\mathbf{x}_nÂ \\mid \\mathbf{x}_{n-1})$$\n",
    "\n",
    "\n",
    "\n",
    "## Information theoretic measures of musical expectation\n",
    "\n",
    "* Information Content: Unexpectedness of a musical event\n",
    "\n",
    "$$\\text{IC}(\\mathbf{x}_n\\mid \\mathbf{x}_{n-1}, \\dots) = -\\log_2 p(\\mathbf{x}_n \\mid \\mathbf{x}_{n-1}, \\dots )$$\n",
    "\n",
    "* Entropy: How uncertain is a musical event\n",
    "\n",
    "$$\\text{H}(\\mathbf{x}_{n-1:1})= \\sum_{i \\in \\mathbf{A}} p(\\mathbf{x}_i \\mid \\mathbf{x}_{n-1}, \\dots) \\text{IC}(\\mathbf{x}_i\\mid \\mathbf{x}_{n-1}, \\dots)$$\n",
    "\n",
    "where $\\mathbf{A}$ is the set of possible states that $\\mathbf{x}$ can take.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ea261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Uncomment this line if the kernel keeps crashing\n",
    "# See https://stackoverflow.com/a/53014308\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "import numpy as np\n",
    "import partitura as pt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea836bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "# To filter out short melodies by the minimum number of notes that a sequence should have\n",
    "\n",
    "min_seq_len = 10\n",
    "sequences = load_data(min_seq_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a598ae3b",
   "metadata": {},
   "source": [
    "## Tasks 1: Data loading & preparation\n",
    "1. check out the content of the variable \"sequences\", if unclear have a look at the loading function.\n",
    "2. which musical texture do these sequences exhibit? (https://en.wikipedia.org/wiki/Texture_(music))\n",
    "3. write a function to derive sequences of pitches from this data.\n",
    "4. write a function to derive sequences of durations from this data. Modify this to compute inter onset intervals (IOIs; the time between two consecutive onsets). Can you encode rests as well by comparing duration with IOI? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a17d83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = list()\n",
    "for seq in sequences:\n",
    "    # pitch\n",
    "    # features.append(seq[\"pitch\"])\n",
    "    # pitch class\n",
    "    features.append(np.mod(seq[\"pitch\"], 12))\n",
    "    # features.append(seq[\"duration_sec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "enc.fit(np.hstack(features))\n",
    "encoded_sequences = [enc.transform(seq) for seq in features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa7a8b9b",
   "metadata": {},
   "source": [
    "## Tasks 2: Data exploration:\n",
    "\n",
    "1. compute and draw a histogram of pitches. Modify this to show pitch classes!\n",
    "2. compute and draw a histogram of IOIs. The input MIDI files are deadpan, i.e. the IOIs in seconds correspond to the notated duration exactly. Look through the IOIs and make an educated guess for some smallest float time unit that could serve as integer smallest time division. Encode the IOIs as multiples of this smallest integer. Which multiples make musical sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ede8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "alphabet_size = len(enc.classes_)\n",
    "alphabet = enc.classes_\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "print(f\"Unique elements {alphabet_size}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(\n",
    "    np.hstack(encoded_sequences),\n",
    "    bins=alphabet_size,\n",
    "    range=(0, alphabet_size),\n",
    "    color=\"firebrick\",\n",
    ")\n",
    "ax.set_xticks(np.arange(alphabet_size) + 0.5)\n",
    "ax.set_xticklabels(alphabet)\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a674b16f",
   "metadata": {},
   "source": [
    "## Tasks 3; A Markov Chain:\n",
    "\n",
    "1. choose a data type to model: pitch, pitch class, IOIs, or durations (including or without an encoding for rests). Concatenate all the sequences into one long data sequence.\n",
    "\n",
    "2. You have now a sequence **X** of symbols from an alphabet **A** (set of possible symbols of your chosen data type):\n",
    "\n",
    "$$ \\mathbf{X} = \\{\\mathbf{x_0}, \\dots, \\mathbf{x_n} \\mid \\mathbf{x}_{i} \\in  \\mathbf{A} \\forall i \\in 0, \\dots, n \\}$$\n",
    "\n",
    "Compute the empirical conditional probability of seeing any symbol after just having seen any other:\n",
    "\n",
    "$$ p(\\mathbf{x_i}\\mid \\mathbf{x_{i-1}}) $$\n",
    "\n",
    "What is the dimensionality of this probability  given $\\lvert A \\rvert = d $? Do you recall what this probability was called in the context of HMMs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5368b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def maximum_likelihood_fom(sequences, alphabet_size):\n",
    "    probs = np.zeros((alphabet_size, alphabet_size)) + 1e-6\n",
    "    for seq in sequences:\n",
    "        for p1, p2 in zip(seq[:-1], seq[1:]):\n",
    "            probs[p1, p2] += 1\n",
    "    probsum = np.sum(probs, axis=1)\n",
    "    normalized_distribution = (probs.T / probsum).T\n",
    "    return normalized_distribution\n",
    "\n",
    "\n",
    "transition_probabilities = maximum_likelihood_fom(encoded_sequences, alphabet_size)\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "a = ax.matshow(transition_probabilities, aspect=\"equal\", cmap=\"BuPu\")\n",
    "ax.set_xticks(range(alphabet_size))\n",
    "ax.set_xticklabels(enc.classes_)\n",
    "ax.set_yticks(range(alphabet_size))\n",
    "ax.set_yticklabels(enc.classes_)\n",
    "\n",
    "plt.colorbar(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9eaa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def probability_of_sequence(\n",
    "    sequence,\n",
    "    transition_probs=transition_probabilities,\n",
    "    alphabet_size=alphabet_size,\n",
    "):\n",
    "    \"\"\"\n",
    "    p(x_1,...,x_n)\n",
    "    \"\"\"\n",
    "    p_0 = 1 / alphabet_size\n",
    "    transitions = np.prod(\n",
    "        [transition_probs[i, j] for i, j in zip(sequence[1:], sequence[:-1])]\n",
    "    )\n",
    "    prob = p_0 * transitions\n",
    "    return prob\n",
    "\n",
    "\n",
    "def information_content_fom(\n",
    "    sequence,\n",
    "    transition_probs=transition_probabilities,\n",
    "    alphabet_size=alphabet_size,\n",
    "):\n",
    "    \"\"\"\n",
    "    For first order Markov models\n",
    "\n",
    "    IC(x_n|x_{n-1},...,x_1) = IC(x_n|x_{n-1}) = - log2(p(x_n | x_{n-1}))\n",
    "    \"\"\"\n",
    "    ic = -np.log2(transition_probs[sequence[-1], sequence[-2]])\n",
    "    return ic\n",
    "\n",
    "\n",
    "def entropy_fom(sequence, alphabet=alphabet, transition_probs=transition_probabilities):\n",
    "    \"\"\"\n",
    "    Entropy for first order Markov model\n",
    "    \"\"\"\n",
    "    entropy_components = []\n",
    "    for al in alphabet:\n",
    "        in_seq = np.zeros_like(sequence)\n",
    "        in_seq[:-1] = sequence[:-1]\n",
    "        in_seq[-1] = al\n",
    "\n",
    "        ic = information_content_fom(\n",
    "            in_seq, transition_probs, alphabet_size=len(alphabet)\n",
    "        )\n",
    "        prob = transition_probs[al, sequence[-2]]\n",
    "\n",
    "        entropy_components.append(ic * prob)\n",
    "    entropy = np.sum(entropy_components)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d44c602",
   "metadata": {},
   "source": [
    "## Tasks 4: Markov Chain Generation:\n",
    "\n",
    "1. By computing the probability $ \\mathbb{P}(\\mathbf{x_i}\\mid \\mathbf{x_{i-1}}) $ in task 3 you have fully specified a discrete-time finite state space Markov Chain model (https://en.wikipedia.org/wiki/Discrete-time_Markov_chain)! Given an initial symbol \"s_0\", you can generate the subsequent symbols by sampling from the conditional probability distribution\n",
    "\n",
    "$$ p(\\mathbf{x_i}\\mid \\mathbf{x_{i-1}} = \\mathbf{s_{0}}) $$\n",
    "\n",
    "Write a function that samples from a finite state space given an input probability distribution.\n",
    "\n",
    "2. Use the previously defined function and the Markov Chain to write a sequence generator based on an initial symbol.\n",
    "3. Start several \"walkers\", i.e. sampled/generated sequences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bcf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from partitura.utils.synth import synthesize, SAMPLE_RATE\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "def sample(distribution):\n",
    "    cs = distribution.cumsum()\n",
    "    samp = np.random.rand(1)\n",
    "    return list(samp < cs).index(True)\n",
    "\n",
    "\n",
    "def generate(start=0, length=100):\n",
    "    melody = [start]\n",
    "    for k in range(length - 1):\n",
    "        melody.append(sample(transition_probabilities[melody[-1], :]))\n",
    "    return np.array(melody)\n",
    "\n",
    "\n",
    "def synthesize_generated_sequence(generated_sequence):\n",
    "    note_array = np.zeros(\n",
    "        len(generated_sequence),\n",
    "        dtype=[\n",
    "            (\"onset_sec\", \"f4\"),\n",
    "            (\"duration_sec\", \"f4\"),\n",
    "            (\"pitch\", \"i4\"),\n",
    "            (\"velocity\", \"i4\"),\n",
    "            (\"id\", \"U256\"),\n",
    "        ],\n",
    "    )\n",
    "    note_array[\"pitch\"] = generated_sequence + 60\n",
    "    note_array[\"onset_sec\"] = np.arange(len(generated_sequence)) * 0.5\n",
    "    note_array[\"duration_sec\"] += 0.5\n",
    "    note_array[\"velocity\"] = 64\n",
    "    note_array[\"id\"] = np.array([f\"n{i}\" for i in range(len(generated_sequence))])\n",
    "\n",
    "    signal = synthesize(note_array, harmonic_dist=\"shepard\")\n",
    "    ipd.display(ipd.Audio(data=signal, rate=SAMPLE_RATE, normalize=False))\n",
    "\n",
    "    return note_array\n",
    "\n",
    "\n",
    "n_notes = 100\n",
    "generated_sequence = generate(length=n_notes)\n",
    "print(generated_sequence)\n",
    "note_array = synthesize_generated_sequence(generated_sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbe6b324",
   "metadata": {},
   "source": [
    "## Tasks 5: n-gram Context Model:\n",
    "\n",
    "1. The Markov Chains used until now have only very limited memory. In fact, they only ever know the last played pitch or duration. Longer memory models can be created by using the conditional probability of any new symbol based on an n-gram context of the symbol (https://en.wikipedia.org/wiki/N-gram):\n",
    "$$ p(\\mathbf{x_i}\\mid \\mathbf{x_{i-1}}, \\dots, \\mathbf{x_{i-n}}) $$\n",
    "\n",
    "This probability will generally not look like a matrix anymore, but we can easily encode it as a dictionary. Write a function that creates a 3-gram context model from the data sequence **X**!\n",
    "\n",
    "2. The longer the context, the more data we need to get meaningful or even existing samples for all contexts (note that the number of different contexts grows exponentially with context length). What could we do to approximate the distribution for unseen contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "\n",
    "def create_context_model(sequences, n, n_states=alphabet_size):\n",
    "    a_priori_probability = np.ones(n_states) / n_states\n",
    "    context_model = defaultdict(lambda: copy.copy(a_priori_probability))\n",
    "    for sequence in sequences:\n",
    "        for idx in range(len(sequence) - n):\n",
    "            local_string = \"\"\n",
    "            for p in sequence[idx : idx + n]:\n",
    "                local_string += str(p)\n",
    "            context_model[local_string][sequence[idx + n]] += 1\n",
    "\n",
    "    for key in context_model.keys():\n",
    "        prob_dist = context_model[key]\n",
    "        context_model[key] = prob_dist / prob_dist.sum()\n",
    "\n",
    "    return context_model\n",
    "\n",
    "\n",
    "cm = create_context_model(encoded_sequences, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_context_model(start=[0, 0, 0], length=100, context_model=cm):\n",
    "    melody = start\n",
    "    for k in range(length):\n",
    "        key = \"\"\n",
    "        for p in melody[-len(start) :]:\n",
    "            key += str(p)\n",
    "        melody.append(sample(context_model[key]))\n",
    "    return np.array(melody)\n",
    "\n",
    "\n",
    "generated_sequence = generate_with_context_model(start=[0, 2, 4, 5, 7])\n",
    "\n",
    "note_array = synthesize_generated_sequence(generated_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
