{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MusicalInformatics/miws23/blob/main/expectation/intro_to_deep_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-candle",
   "metadata": {},
   "source": [
    "# A Very Quick and Dirty Introduction to Deep Learning and PyTorch\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "Deep learning is a family of machine learning techniques that is the extension of neural networks. In this case, the word *deep* refers to the fact that these neural networks are organized into many **layers**.\n",
    "\n",
    "This short tutorial aims to be a practical introduction to deep learning using [PyTorch](https://pytorch.org/). **Parts of this tutorial are taken from PyTorch's [Introduction to PyTorch](https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-paste",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Most computations on neural networks computations are linear algebra operations on **tensors**. Tensors can be thought as a generalization of matrices (the proper mathematical definition of tensors is a [bit more complicated](https://en.wikipedia.org/wiki/Tensor)). \n",
    "\n",
    "A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor, and so on. PyTorch is built around tensors!\n",
    "\n",
    "##### Creating Tensors\n",
    "We'll start with a few basic tensor manipulations. We start with ways of creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = torch.zeros(5, 3)\n",
    "print(z)\n",
    "print(z.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-august",
   "metadata": {},
   "source": [
    "It is possible to override the default datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-addition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = torch.ones((5, 3), dtype=torch.int16)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-queensland",
   "metadata": {},
   "source": [
    "We can also initialize tensors with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1729)\n",
    "r1 = torch.rand(2, 2)\n",
    "print('A random tensor:')\n",
    "print(r1)\n",
    "\n",
    "r2 = torch.rand(2, 2)\n",
    "print('\\nA different random tensor:')\n",
    "print(r2) # new values\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "r3 = torch.rand(2, 2)\n",
    "print('\\nShould match r1:')\n",
    "print(r3) # repeats values of r1 because of re-seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-joint",
   "metadata": {},
   "source": [
    "We can initialize a tensor from a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-recognition",
   "metadata": {},
   "source": [
    "And convert the tensors back to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_tensor = t.numpy()\n",
    "print(f'numpy_tensor type {type(numpy_tensor)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-boards",
   "metadata": {},
   "source": [
    "##### Basic Arithmetic and Elementwise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "twos = torch.ones(2, 3) * 2 # every element is multiplied by 2\n",
    "print(twos)\n",
    "\n",
    "threes = ones + twos       # additon allowed because shapes are similar\n",
    "print(threes)              # tensors are added element-wise\n",
    "print(threes.shape)        # this has the same dimensions as input tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-claim",
   "metadata": {},
   "source": [
    "**Exercise**: The following snippet results in a runtime error. Why is that? (you have to uncomment the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = torch.rand(2, 3)\n",
    "r2 = torch.rand(3, 2)\n",
    "\n",
    "# r3 = r1 + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-february",
   "metadata": {},
   "source": [
    "Elementwise operations and aggregate operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.rand(2, 2) - 0.5 * 2 # values between -1 and 1\n",
    "print('A random matrix, r:')\n",
    "print(r)\n",
    "\n",
    "# Common mathematical operations are supported:\n",
    "print('\\nAbsolute value of r:')\n",
    "print(torch.abs(r))\n",
    "\n",
    "# ...as are trigonometric functions:\n",
    "print('\\nInverse sine of r:')\n",
    "print(torch.asin(r))\n",
    "\n",
    "# ...and statistical and aggregate operations:\n",
    "print('\\nAverage and standard deviation of r:')\n",
    "print(torch.std_mean(r))\n",
    "print('\\nMaximum value of r:')\n",
    "print(torch.max(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-excellence",
   "metadata": {},
   "source": [
    "##### Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-honolulu",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r1 = torch.rand(2, 2) - 0.5 * 2 # values between -1 and 1\n",
    "print('A random matrix, r1:')\n",
    "print(r)\n",
    "r2 = torch.rand(2, 2) - 0.5 * 3 # values between -1.5 and 1.5\n",
    "print('A random matrix, r2:')\n",
    "print(r2)\n",
    "\n",
    "print('\\nMatrix Multiplication of r1 and r2')\n",
    "print(torch.matmul(r1, r2))\n",
    "print('\\nDeterminant of r1:')\n",
    "print(torch.det(r1))\n",
    "print('\\nSingular value decomposition of r1:')\n",
    "print(torch.svd(r1))\n",
    "print('\\nPseudo-inverse of r1:')\n",
    "print(torch.pinverse(r1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-illinois",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks\n",
    "\n",
    "Neural networks have its origins in early work that tried to model biological networks of neurons in the brain [(McCulloch and Pits, 1943)](https://homes.luddy.indiana.edu/jbollen/I501F13/readings/mccullochpitts1943.pdf). For this reason, these methods are called neural networks, although the resemblance to real neural cells is just superficial.\n",
    "\n",
    "We can understand artificial neural networks (or simply neural networks) as **complex compositions of simpler functions**. Each node within a network is called a **unit**, and each of these units calculates a weighted sum of the inputs from predecessor nodes and then applies a nonlinear function.\n",
    "\n",
    "Let us consider the following simple case:\n",
    "\n",
    "Let $a_j$ denote the output of unit $j$ can be computed as\n",
    "\n",
    "$$a_j = g_j\\left(\\sum_{i} w_{ij}a_i + b_j\\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "* $g_j(\\cdot)$ is a nonlinear **activation function** associated with unit $j$\n",
    "* $w_{ij}$ is the weight attached to the link from unit $i$ to unit $j$\n",
    "* $b_j$ is a scalar bias\n",
    "\n",
    "with this convention, we can write the above equation in vector form as\n",
    "\n",
    "$$a_j = g_j\\left(\\mathbf{w}_j^T\\mathbf{x} + b_j\\right)$$\n",
    "\n",
    "where $\\mathbf{w}_j^T$ is the vector of weights leading into unit $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-connecticut",
   "metadata": {},
   "source": [
    "##### Activation Functions\n",
    "\n",
    "Some of the most common activation functions are\n",
    "\n",
    "* **Sigmoid** function\n",
    "$$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_numpy(x):\n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "    \n",
    "x = np.linspace(-3, 3)\n",
    "\n",
    "sig_numpy = sigmoid_numpy(x)\n",
    "\n",
    "plt.plot(x, sig_numpy)\n",
    "plt.ylim((0, 1))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-auditor",
   "metadata": {},
   "source": [
    "* **ReLU** (rectified linear units)\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_numpy(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.linspace(-3, 3)\n",
    "\n",
    "relu = relu_numpy(x)\n",
    "\n",
    "plt.plot(x, relu)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-pottery",
   "metadata": {},
   "source": [
    "* **Hyperbolic tangent**\n",
    "\n",
    "$$\\tanh(x) = \\frac{\\exp(2x) - 1}{\\exp(2x) + 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3)\n",
    "tanh = np.tanh(x)\n",
    "plt.plot(x, tanh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-dutch",
   "metadata": {},
   "source": [
    "### Computation Graphs\n",
    "\n",
    "In the following we will consider $\\mathbf{x}$ an input (training or test) example), $\\hat{\\mathbf{y}}$ are the outputs of the network and $\\mathbf{y}$ the *true values* to derive a learning signal\n",
    "\n",
    "* **Input Encoding**: It depends on the problem we want to model. Assume that we have $n$ input nodes\n",
    "    * If we have Boolean inputs, *false* is usually mapped to $0$ and *true* to $1$, although sometimes $-1$ and $1$ are used\n",
    "    * If we have real valued inputs, we can just use the actual values, although it is common to scale the inputs to fit a fixed range, or use a transformation like a log scale if the magnitudes of the different examples vary a lot.\n",
    "    * If we have categorical encodings, we can use a *one-hot* encoding\n",
    "    \n",
    "* **Output Layers and Loss Function**: On the output side of the network, the problem of encoding raw data values into actual values $\\mathbf{y}$ is very similar than the input encoding: We can use a numerical mapping for Boolean outputs, (scaled/transformed) real values for real-valued outputs, one-hot encodings for categorical data. This can be achieved by choosing an appropriate output nonlinearity:\n",
    "    * For Boolean outputs, we can use the sigmoid function (if we are mapping *false* and *true* to 0 and 1, respectively), or tanh (if we are mapping to -1 and 1).\n",
    "    * For categorical problems, we can use a **softmax** layer:\n",
    "    $$\\text{softmax}(\\mathbf{in})_k = \\frac{\\exp(in_k)}{\\sum_{l=1}^{d} \\exp(in_l)}$$\n",
    "    where $\\mathbf{in} = (in_1, \\dots, in_d)$ are the input values.\n",
    "    * For regression problems, it is usual to use the identity function $g(x) = x$\n",
    "    * And many more, depending on the problem (e.g., mixture density layers).\n",
    "    \n",
    "* **Hidden Layers**: We can think of the hidden layers as learning different *representations* for the input $\\mathbf{x}$. In many cases, the $l$-th hidden layer will be given as a function of the previous layers:\n",
    "\n",
    "$$\\mathbf{h}_l(\\mathbf{h}_{l-1}) = g_l(\\mathbf{W}\\mathbf{h}_{l-1} + \\mathbf{b}_l)$$\n",
    "\n",
    "although this form would depend on the particular neural architecture. (the above example would be for a fully connected feed forward neural network). With this notation, we can write the inputs as the $0$-th layer $\\mathbf{h}_0 = \\mathbf{x}$ and the outputs as the $L$-th layer, i.e., $\\hat{\\mathbf{y}} = \\mathbf{h}_L(\\mathbf{h}_{l-1})$.\n",
    "\n",
    "##### Training the network\n",
    "\n",
    "The **loss function** is a measure of how good the predictions of the network are, i.e., how close do the predictions of the network approximate the expected values $\\mathbf{y}$. We can use this loss function to learn the parameters of the network (the sets of weights and biases) as those which minimize the loss function\n",
    "\n",
    "$$\\hat{\\mathbf{\\theta}} = \\arg \\min_{\\mathbf{\\theta}} \\mathcal{L}(\\mathbf{Y}, \\hat{\\mathbf{Y}})$$\n",
    "\n",
    "For regression problems, it is common to use the mean squared error\n",
    "\n",
    "$$\\text{mse}(\\mathbf{Y}, \\hat{\\mathbf{Y}}) = \\frac{1}{N} \\sum_{i}||\\mathbf{y}_i - \\hat{\\mathbf{y}}_i||^2$$\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
