{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MusicalInformatics/miws23/blob/main/key_meter/meter_estimation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install dependencies\n",
    "    ! pip install python-hiddenmarkov\n",
    "    ! pip install ipympl\n",
    "    ! pip install partitura\n",
    "\n",
    "    # Enable interactive plots in colab\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "    # To be able to access helper modules in the repo for this tutorial\n",
    "    # (not necessary if the jupyter notebook is run locally instead of google colab)\n",
    "    !git clone https://github.com/MusicalInformatics/miws23\n",
    "    import sys\n",
    "    sys.path.insert(0, \"/content/miws23/key_meter/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meter and Tempo Estimation on Symbolic Music\n",
    "\n",
    "Meter, time signature, beat, downbeat, and tempo estimation form a group of related tasks. At first glance, many of them might seem trivial on symbolic music, after all the precise onsets of notes are known from the MIDI data. But with changing tempi and unknown measures it can be quite challenging! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some stuff\n",
    "%matplotlib widget\n",
    "from typing import Union, Tuple, Iterable\n",
    "\n",
    "import partitura as pt\n",
    "from partitura.performance import PerformanceLike\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from meter_estimation_utils import NOTEBOOK_DIR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before describing the methods for computing metrical and tempo information, let's do some data exploration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load piece\n",
    "path_to_match = os.path.join(NOTEBOOK_DIR, \"example_data\", \"mozart_k265_var1.match\")\n",
    "performance, alignment, score = pt.load_match(path_to_match, create_score=True)\n",
    "ppart = performance[0]\n",
    "note_array = performance.note_array()\n",
    "\n",
    "note_array_score = score.note_array(include_time_signature=True)\n",
    "\n",
    "# We assume that there is just a single time signature throughout the piece\n",
    "time_signature = note_array_score[[\"ts_beats\", \"ts_beat_type\"]][0]\n",
    "\n",
    "print(time_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the tempo of this performance from the alignment information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ptime_to_stime_map,\n",
    "    stime_to_ptime_map,\n",
    ") = pt.musicanalysis.performance_codec.get_time_maps_from_alignment(\n",
    "    ppart_or_note_array=note_array,\n",
    "    spart_or_note_array=note_array_score,\n",
    "    alignment=alignment,\n",
    ")\n",
    "\n",
    "# unique score onsets\n",
    "s_onsets = np.unique(note_array_score[\"onset_beat\"])\n",
    "# aggregated performed onsets\n",
    "p_onsets = stime_to_ptime_map(s_onsets)\n",
    "\n",
    "tempo_curve = 60 * np.diff(s_onsets) / np.diff(p_onsets)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s_onsets[:-1], tempo_curve, color=\"firebrick\")\n",
    "ax.set_xlabel(\"Score time (beats)\")\n",
    "ax.set_ylabel(\"Tempo (bpm)\")\n",
    "ax.set_title(f\"Average Tempo {tempo_curve.mean():.2f} bpm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_div = 16\n",
    "# Piano roll\n",
    "piano_roll = pt.utils.music.compute_pianoroll(\n",
    "    note_info=note_array,\n",
    "    time_unit=\"sec\",\n",
    "    time_div=time_div,\n",
    "    note_separation=True,\n",
    ").toarray()\n",
    "\n",
    "# Frames of the piano roll with onsets (velocity is added)\n",
    "frames_nq = (\n",
    "    pt.utils.music.compute_pianoroll(\n",
    "        note_info=note_array,\n",
    "        time_unit=\"sec\",\n",
    "        time_div=time_div,\n",
    "        onset_only=True,\n",
    "        binary=True,  # Count number of notes, instead of velocity\n",
    "    )\n",
    "    .toarray()\n",
    "    .sum(0)\n",
    ")\n",
    "#\n",
    "# frames_nq[frames_nq != 0] = 1\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=1,\n",
    "    figsize=(8, 8),\n",
    "    sharex=True,\n",
    ")\n",
    "ax[0].imshow(\n",
    "    piano_roll,\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"binary\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "ax[1].set_xlabel(\"Time (s)\")\n",
    "ax[0].set_ylabel(\"MIDI pitch\")\n",
    "ax[1].set_ylabel(\"Simultaneous played notes\")\n",
    "ax[1].set_xticks(\n",
    "    np.arange(0, len(frames_nq), 100),\n",
    "    np.arange(0, len(frames_nq), 100) / time_div,\n",
    ")\n",
    "ax[1].plot(frames_nq, color=\"firebrick\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration - 1\n",
    "\n",
    "Let's have a look at the distribution of inter onset intervals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOIs = np.diff(np.sort(note_array[\"onset_sec\"]))\n",
    "hist, bins = np.histogram(IOIs, bins=100)\n",
    "\n",
    "print(60 / (4 * bins[np.argmax(hist)]))\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(IOIs, bins=bins)\n",
    "ax.set_xlabel(\"IOI (s)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration - 2\n",
    "\n",
    "There are many small inter onset intervals from musical notes that are played \"simultaneously\". For the task at hand, the spread or local deviations are not of interest, on the other hand, we would like to derive a common onset from notes that are notated in the same score position. Let's give it a try by converting the measured onsets to a framed unidimensional signal that contains the number of onsets in a performance in a small window in time, e.g. 20 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMERATE = 50\n",
    "\n",
    "onset_min = note_array[\"onset_sec\"].min()\n",
    "onset_max = note_array[\"onset_sec\"].max()\n",
    "onset_duration = onset_max - onset_min\n",
    "frames = np.zeros(int(onset_duration * FRAMERATE) + 1)\n",
    "for note in note_array:\n",
    "    frames[int((note[\"onset_sec\"] - onset_min) * FRAMERATE)] += 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(frames)) / FRAMERATE, frames)\n",
    "ax.set_xlabel(\"Time (sec)\")\n",
    "ax.set_ylabel(\"Note count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration - 3\n",
    "\n",
    "The previous aggregation is fairly noisy and for most cases does not give good results, reason being the arbitrary cutoff in onset times between the frames. Let's try different frame generation process that aggregates very close onsets and at the same time encodes dynamics / quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_notes = [(0, 0)]\n",
    "CHORD_SPREAD_TIME = 1 / 12  # a little faster than 16th notes at 180 bpm\n",
    "\n",
    "for note in ppart.notes:\n",
    "    prev_note_on = aggregated_notes[-1][0]\n",
    "    note_on = note[\"note_on\"]\n",
    "    prev_note_vel = aggregated_notes[-1][1]\n",
    "    if abs(note_on - prev_note_on) < CHORD_SPREAD_TIME:\n",
    "        aggregated_notes[-1] = (note_on, prev_note_vel + 1)\n",
    "        # aggregated_notes[-1] = (note_on, prev_note_vel + note[\"velocity\"])  # 1\n",
    "    else:\n",
    "        aggregated_notes.append((note_on, 1))  # 1\n",
    "        # aggregated_notes.append((note_on, note[\"velocity\"]))  # 1\n",
    "\n",
    "frames_a = np.zeros(int(onset_duration * FRAMERATE) + 1)\n",
    "for note in aggregated_notes:\n",
    "    frames_a[int((note[0] - onset_min) * FRAMERATE)] += note[1]\n",
    "\n",
    "frames_a[frames_a < 0.1] = 0.0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    np.arange(len(frames)) / FRAMERATE,\n",
    "    frames_a,\n",
    "    label=\"cordified\",\n",
    "    color=\"firebrick\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax.plot(\n",
    "    np.arange(len(frames)) / FRAMERATE,\n",
    "    frames,\n",
    "    label=\"quantized\",\n",
    "    color=\"navy\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration - 4\n",
    "\n",
    "Now that signal looks a lot more like approximately isochronous onsets, some with high velocity (sum of velocities occurring at this onset), some with lower. You can change the velocity to just a note count per onset to get an alternative onset value. \n",
    "\n",
    "One standard signal processing way of getting an estimate of the frequency of these onsets is to compute autocorrelation with increasing lag parameters. We plot the accumulated signal over time for different lag parameters to see at which lag the signal reinforces itself most strongly. How do we interpret these dimensions? The number of frames between onsets (= lag) is inversely proportional to the tempo of the onsets!\n",
    "\n",
    "Besides visual analysis we can try some basic peak picking to get a numeric estimate of the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meter_estimation_utils import compute_autocorrelation\n",
    "\n",
    "autocorr = compute_autocorrelation(frames_nq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(autocorr, color=\"firebrick\")\n",
    "ax.set_xlabel(\"Lag\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "# see https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html\n",
    "a, _ = find_peaks(autocorr[1:], prominence=30)\n",
    "m = np.argmax(autocorr[1:])\n",
    "print(60 / ((a + 1) / time_div), 60 / ((m + 1) / time_div))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## HMMs for Meter Estimation\n",
    "\n",
    "Without more sophisticated methods it's hard to get a robust estimate of meter and tempo from this data.\n",
    "\n",
    "A hidden model with a constant transition model and a state space encoding a measure in frames (1 measure = 100 states = 100 frames). The observation model produces onsets at states that encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiddenmarkov import HMM, ConstantTransitionModel, ObservationModel\n",
    "\n",
    "FRAMERATE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeterObservationModel(ObservationModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: int = 100,\n",
    "        downbeat_idx: Iterable = [0],\n",
    "        beat_idx: Iterable = [50],\n",
    "        subbeat_idx: Iterable = [25],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.states = states\n",
    "        # observation 1 = note onset present, 0 = nothing present\n",
    "        self.probabilities = np.ones((2, states)) / 100\n",
    "        self.probabilities[0, :] = 0.99\n",
    "        for idx in subbeat_idx:\n",
    "            self.probabilities[:, idx] = [0.5, 0.5]\n",
    "        for idx in beat_idx:\n",
    "            self.probabilities[:, idx] = [0.3, 0.7]\n",
    "        for idx in downbeat_idx:\n",
    "            self.probabilities[:, idx] = [0.1, 0.9]\n",
    "        self.db = downbeat_idx\n",
    "        self.b = beat_idx\n",
    "        self.sb = subbeat_idx\n",
    "\n",
    "    def get_beat_states(self, state_sequence: np.ndarray) -> np.ndarray:\n",
    "        state_encoder = np.zeros_like(state_sequence)\n",
    "        for i, state in enumerate(state_sequence):\n",
    "            if state in self.sb:\n",
    "                state_encoder[i] = 1\n",
    "            if state in self.b:\n",
    "                state_encoder[i] = 2\n",
    "            if state in self.db:\n",
    "                state_encoder[i] = 3\n",
    "        return state_encoder\n",
    "\n",
    "    def __call__(self, observation: np.ndarray) -> np.ndarray:\n",
    "        if not self.use_log_probabilities:\n",
    "            return self.probabilities[observation, :]\n",
    "        else:\n",
    "            return np.log(self.probabilities[observation, :])\n",
    "\n",
    "\n",
    "def getTransitionMatrix(states: int, distribution: Iterable = [0.1, 0.8, 0.1]):\n",
    "    transition_matrix = (\n",
    "        np.eye(states, k=0) * distribution[0]\n",
    "        + np.eye(states, k=1) * distribution[1]\n",
    "        + np.eye(states, k=2) * distribution[2]\n",
    "        + np.ones((states, states)) / 1e7\n",
    "    )\n",
    "    transition_matrix[-2, 0] = distribution[2]\n",
    "    transition_matrix[-1, 0] = distribution[2] + distribution[1]\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "transition_matrix = getTransitionMatrix(states=100, distribution=[0.1, 0.8, 0.1])\n",
    "\n",
    "ax.imshow(transition_matrix, aspect=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHMM(\n",
    "    tempo: float = 50,\n",
    "    frame_rate: int = FRAMERATE,  # frames_per_beat\n",
    "    beats_per_measure: int = 4,\n",
    "    subbeats_per_beat: int = 2,\n",
    "):\n",
    "    frames_per_beat = 60 / tempo * frame_rate\n",
    "    frames_per_measure = frames_per_beat * beats_per_measure\n",
    "    states = int(frames_per_measure)\n",
    "    downbeat_idx = [0]\n",
    "    beat_idx = [int(states / beats_per_measure * k) for k in range(beats_per_measure)]\n",
    "    subbeat_idx = [\n",
    "        int(states / (beats_per_measure * subbeats_per_beat) * k)\n",
    "        for k in range(beats_per_measure * subbeats_per_beat)\n",
    "    ]\n",
    "\n",
    "    observation_model = MeterObservationModel(\n",
    "        states=states,\n",
    "        downbeat_idx=downbeat_idx,\n",
    "        beat_idx=beat_idx,\n",
    "        subbeat_idx=subbeat_idx,\n",
    "    )\n",
    "\n",
    "    transition_matrix = getTransitionMatrix(states)\n",
    "    transition_model = ConstantTransitionModel(transition_matrix)\n",
    "\n",
    "    return observation_model, transition_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_model, transition_model = createHMM(\n",
    "    tempo=60,\n",
    "    frame_rate=FRAMERATE,\n",
    "    beats_per_measure=2,\n",
    "    subbeats_per_beat=2,  # bpm\n",
    ")\n",
    "\n",
    "hmm = HMM(observation_model=observation_model, transition_model=transition_model)\n",
    "\n",
    "frames_a[frames_a < 1.0] = 0\n",
    "frames_a[frames_a >= 1.0] = 1\n",
    "\n",
    "observations = np.array(frames_a, dtype=int)\n",
    "path, log_lik = hmm.find_best_sequence(observations)\n",
    "\n",
    "beat_states = observation_model.get_beat_states(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(beat_states)) / FRAMERATE, beat_states)\n",
    "plt.show()\n",
    "\n",
    "beat_times = np.where(beat_states >= 2)[0] / FRAMERATE\n",
    "\n",
    "print(np.median(60 / np.diff(beat_times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(path[:1000])\n",
    "plt.plot(observations[:1000] * 100)\n",
    "ax.plot(observation_model.get_beat_states(path)[:1000] * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meter_estimation_utils import get_frames_quantized, get_frames_chordify\n",
    "\n",
    "\n",
    "def meter_identification(\n",
    "    note_info: PerformanceLike,\n",
    "    beats_per_measure: Iterable[int]=[2, 3, 4],\n",
    "    subbeats_per_beat: Iterable[int]=[2, 3],\n",
    "    tempi: Union[Iterable[int], str]=\"auto\",\n",
    "    frame_aggregation: str=\"chordify\",\n",
    "    value_aggregation:str=\"num_notes\",\n",
    "    framerate: int=50,\n",
    "    frame_threshold: float=0.0,\n",
    "    chord_spread_time: float=1 / 12,\n",
    "    max_tempo:float=250,\n",
    "    min_tempo:float=30,\n",
    ") -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Meter Identification using HMMs\n",
    "    \"\"\"\n",
    "    # get note array\n",
    "    note_array = pt.utils.ensure_notearray(note_info)\n",
    "\n",
    "    if frame_aggregation == \"chordify\":\n",
    "        frames = get_frames_chordify(\n",
    "            note_array=note_array,\n",
    "            framerate=framerate,\n",
    "            chord_spread_time=chord_spread_time,\n",
    "            aggregation=value_aggregation,\n",
    "            threshold=frame_threshold,\n",
    "        )\n",
    "    elif frame_aggregation == \"quantize\":\n",
    "        frames = get_frames_quantized(\n",
    "            note_array=note_array,\n",
    "            framerate=framerate,\n",
    "            aggregation=value_aggregation,\n",
    "            threshold=frame_threshold,\n",
    "        )\n",
    "\n",
    "    if tempi == \"auto\":\n",
    "        autocorr = compute_autocorrelation(frames)\n",
    "        beat_period, _ = find_peaks(autocorr[1:], prominence=20)\n",
    "        tempi = 60 * framerate / (beat_period + 1)\n",
    "        tempi = tempi[np.logical_and(tempi <= max_tempo, tempi >= min_tempo)]\n",
    "\n",
    "    likelihoods = []\n",
    "\n",
    "    for ts_num in beats_per_measure:\n",
    "        for sbpb in subbeats_per_beat:\n",
    "            for tempo in tempi:\n",
    "                observation_model, transition_model = createHMM(\n",
    "                    tempo=tempo,\n",
    "                    frame_rate=framerate,\n",
    "                    beats_per_measure=ts_num,\n",
    "                    subbeats_per_beat=sbpb,\n",
    "                )\n",
    "\n",
    "                hmm = HMM(\n",
    "                    observation_model=observation_model,\n",
    "                    transition_model=transition_model,\n",
    "                )\n",
    "\n",
    "                frames_a[frames_a < 1.0] = 0\n",
    "                frames_a[frames_a >= 1.0] = 1\n",
    "\n",
    "                observations = np.array(frames_a, dtype=int)\n",
    "                _, log_lik = hmm.find_best_sequence(observations)\n",
    "\n",
    "                likelihoods.append((ts_num, sbpb, tempo, log_lik))\n",
    "\n",
    "    likelihoods = np.array(likelihoods)\n",
    "\n",
    "    best_result = likelihoods[likelihoods[:, 3].argmax()]\n",
    "\n",
    "    best_ts = int(best_result[0])\n",
    "    best_tempo = best_result[2]\n",
    "\n",
    "    return best_ts, best_tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ts, best_tempo = meter_identification(note_array)\n",
    "\n",
    "print(best_ts, best_tempo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "\n",
    "Both presented approaches as well as the initial aggregation can be tuned and improved upon. Here are a few starting points:\n",
    "\n",
    "- Preprocessing:\n",
    "    - aggregation width\n",
    "    - salience encoding; number of onsets, velocity, duration, ...\n",
    "    - cluster based aggregation which update their means continually\n",
    "\n",
    "- HMM:\n",
    "    - more observation classes based on salience (instead of 0 = no onset and 1 = onset)\n",
    "    - change transition probabilities for more or less flexible timing\n",
    "    - add key and key strength estimation per onset (requires pitches per onset), use this information for downbeat and beat estimation\n",
    "    \n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
